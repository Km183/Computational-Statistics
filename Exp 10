import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
np.random.seed(42)  # For reproducibility
n_samples = 200
X = np.vstack([
    np.random.normal(loc=[0, 0], scale=1, size=(n_samples // 4, 2)),
    np.random.normal(loc=[5, 5], scale=1, size=(n_samples // 4, 2)),
    np.random.normal(loc=[-5, 5], scale=1, size=(n_samples // 4, 2)),
    np.random.normal(loc=[5, -5], scale=1, size=(n_samples // 4, 2))
])
plt.figure(figsize=(12, 10))
plt.subplot(2, 2, 1)
plt.scatter(X[:, 0], X[:, 1], s=50, c='blue', marker='o')
plt.title('Original Data Plot')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
plt.subplot(2, 2, 2)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], s=50, c='green', marker='o')
plt.title('Standardized Data Plot')
plt.xlabel('Feature 1 (Standardized)')
plt.ylabel('Feature 2 (Standardized)')
wcss = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)
plt.subplot(2, 2, 3)
plt.plot(K_range, wcss, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Within-cluster Sum of Squares (WCSS)')
optimal_k = 4  # Based on the elbow method, or you could dynamically choose it
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)
plt.subplot(2, 2, 4)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=50, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title('Clustering Results')
plt.xlabel('Feature 1 (Standardized)')
plt.ylabel('Feature 2 (Standardized)')
plt.tight_layout()
plt.show()
